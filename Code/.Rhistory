# Converting to a Corpus
docs <- Corpus(VectorSource(text_df$text))
# Text Cleaning
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "\n")
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# docs <- tm_map(docs, toSpace, "*")
docs <- tm_map(docs, toSpace, "[^[:alnum:] ]")
# setwd("C:/Users/Jeremy Chia/Documents/GitHub/NLP-SustainabilityReports-FinancialPerformance/")
# characters_remove <- read.csv("characters.csv")
#
# for (i in 1:nrow(characters_remove)){
#   docs <- tm_map(docs, toSpace, characters_remove$X.[i])
#   print(i)
# }
# Text Cleaning
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove english (SMART) common stopwords
docs <- tm_map(docs, removeWords, stopwords("SMART"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("reporting","period","report","fy")) # reporting words
## Remove additional stopwords as identified
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code/1. Before removing additional stopwords")
to_remove <- as.vector(read.csv("2. to_remove.csv",header=T)[[1]])
docs <- tm_map(docs, removeWords, to_remove) # remove company, country, irrelevant words
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text lemmization
docs <- lemmatize_words(docs, dictionary = lexicon::hash_lemmas)
#####
# Compute Document Term Matrix where Word >= minimumFrequency
minimumFrequency <- 10
DTM <- DocumentTermMatrix(docs, control = list(bounds = list(global = c(minimumFrequency, Inf))))
DTM <- DTM[apply(DTM,1,FUN=sum)!=0,]
# create models with different number of topics
result <- ldatuning::FindTopicsNumber(
DTM,
topics = seq(from = 2, to = 50, by = 1),
metrics = c("CaoJuan2009",  "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
verbose = TRUE
)
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code/2. After removing additional stopwords")
write.csv(result,"metrics_v6.csv")
FindTopicsNumber_plot(result)
# number of topics
K <- 11
# set random number generator seed
set.seed(1234)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
topics <- tidy(topicModel, matrix = "beta")
# get the top ten terms for each topic
top_terms <- topics  %>% # take the topics data frame and..
group_by(topic) %>% # treat each topic as a different group
top_n(10, beta) %>% # get the top 10 most informative words
ungroup() %>% # ungroup
arrange(topic, -beta) # arrange words in descending informativeness
### GRAPH OUT TOP TERMS
top_terms %>% # take the top terms
mutate(term = reorder(term, beta)) %>% # sort terms by beta value
ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
geom_col(show.legend = FALSE) + # as a bar plot
facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
labs(x = NULL, y = "Beta") + # no x label, change y label
coord_flip() # turn bars sideways
# number of topics
K <- 12
# set random number generator seed
set.seed(1234)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
topics <- tidy(topicModel, matrix = "beta")
# get the top ten terms for each topic
top_terms <- topics  %>% # take the topics data frame and..
group_by(topic) %>% # treat each topic as a different group
top_n(10, beta) %>% # get the top 10 most informative words
ungroup() %>% # ungroup
arrange(topic, -beta) # arrange words in descending informativeness
### GRAPH OUT TOP TERMS
top_terms %>% # take the top terms
mutate(term = reorder(term, beta)) %>% # sort terms by beta value
ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
geom_col(show.legend = FALSE) + # as a bar plot
facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
labs(x = NULL, y = "Beta") + # no x label, change y label
coord_flip() # turn bars sideways
write.csv(top_terms,"top_words_v6_12topics.csv",fileEncoding = "utf8")
# number of topics
K <- 7
# set random number generator seed
set.seed(1234)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
topics <- tidy(topicModel, matrix = "beta")
# get the top ten terms for each topic
top_terms <- topics  %>% # take the topics data frame and..
group_by(topic) %>% # treat each topic as a different group
top_n(10, beta) %>% # get the top 10 most informative words
ungroup() %>% # ungroup
arrange(topic, -beta) # arrange words in descending informativeness
### GRAPH OUT TOP TERMS
top_terms %>% # take the top terms
mutate(term = reorder(term, beta)) %>% # sort terms by beta value
ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
geom_col(show.legend = FALSE) + # as a bar plot
facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
labs(x = NULL, y = "Beta") + # no x label, change y label
coord_flip() # turn bars sideways
write.csv(top_terms,"top_words_v6_7topics.csv",fileEncoding = "utf8")
# number of topics
K <- 12
# set random number generator seed
set.seed(1234)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
topics <- tidy(topicModel, matrix = "beta")
# get the top ten terms for each topic
top_terms <- topics  %>% # take the topics data frame and..
group_by(topic) %>% # treat each topic as a different group
top_n(10, beta) %>% # get the top 10 most informative words
ungroup() %>% # ungroup
arrange(topic, -beta) # arrange words in descending informativeness
### GRAPH OUT TOP TERMS
top_terms %>% # take the top terms
mutate(term = reorder(term, beta)) %>% # sort terms by beta value
ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
geom_col(show.legend = FALSE) + # as a bar plot
facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
labs(x = NULL, y = "Beta") + # no x label, change y label
coord_flip() # turn bars sideways
# number of topics
K <- 12
# set random number generator seed
set.seed(1234)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
topics <- tidy(topicModel, matrix = "beta")
# get the top ten terms for each topic
top_terms <- topics  %>% # take the topics data frame and..
group_by(topic) %>% # treat each topic as a different group
top_n(10, beta) %>% # get the top 10 most informative words
ungroup() %>% # ungroup
arrange(topic, -beta) # arrange words in descending informativeness
### GRAPH OUT TOP TERMS
top_terms %>% # take the top terms
mutate(term = reorder(term, beta)) %>% # sort terms by beta value
ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
geom_col(show.legend = FALSE) + # as a bar plot
facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
labs(x = NULL, y = "Beta") + # no x label, change y label
coord_flip() # turn bars sideways
write.csv(top_terms,"top_words_v6_17topics.csv",fileEncoding = "utf8")
# number of topics
K <- 17
# set random number generator seed
set.seed(1234)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
topics <- tidy(topicModel, matrix = "beta")
# get the top ten terms for each topic
top_terms <- topics  %>% # take the topics data frame and..
group_by(topic) %>% # treat each topic as a different group
top_n(10, beta) %>% # get the top 10 most informative words
ungroup() %>% # ungroup
arrange(topic, -beta) # arrange words in descending informativeness
### GRAPH OUT TOP TERMS
top_terms %>% # take the top terms
mutate(term = reorder(term, beta)) %>% # sort terms by beta value
ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
geom_col(show.legend = FALSE) + # as a bar plot
facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
labs(x = NULL, y = "Beta") + # no x label, change y label
coord_flip() # turn bars sideways
write.csv(top_terms,"top_words_v6_17topics.csv",fileEncoding = "utf8")
# number of topics
K <- 7
# set random number generator seed
set.seed(1234)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
topics <- tidy(topicModel, matrix = "beta")
# get the top ten terms for each topic
top_terms <- topics  %>% # take the topics data frame and..
group_by(topic) %>% # treat each topic as a different group
top_n(10, beta) %>% # get the top 10 most informative words
ungroup() %>% # ungroup
arrange(topic, -beta) # arrange words in descending informativeness
### GRAPH OUT TOP TERMS
top_terms %>% # take the top terms
mutate(term = reorder(term, beta)) %>% # sort terms by beta value
ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
geom_col(show.legend = FALSE) + # as a bar plot
facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
labs(x = NULL, y = "Beta") + # no x label, change y label
coord_flip() # turn bars sideways
### FORM TOPICS
tmResult <- posterior(topicModel)
beta <- tmResult$terms
theta <- tmResult$topics
terms(topicModel, 10)
exampleTermData <- terms(topicModel, 10)
top5termsPerTopic <- terms(topicModel, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")
text_df %>% select(Company, Year)
theta_df <- data.frame(theta)
theta_df$Company <- text_df$Company
theta_df$Year <- text_df$Year
theta_df$page <- text_df$page
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code/2. After removing additional stopwords")
write.csv(theta_df, "topic_distribution_v6_7topics.csv",row.names=F)
# Compute Document Term Matrix where Word >= minimumFrequency
minimumFrequency <- 1
DTM <- DocumentTermMatrix(docs, control = list(bounds = list(global = c(minimumFrequency, Inf))))
DTM <- DTM[apply(DTM,1,FUN=sum)!=0,]
# number of topics
K <- 7
# set random number generator seed
set.seed(1234)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
minimumFrequency <- 0
DTM <- DocumentTermMatrix(docs, control = list(bounds = list(global = c(minimumFrequency, Inf))))
DTM <- DTM[apply(DTM,1,FUN=sum)!=0,]
# number of topics
K <- 7
# set random number generator seed
set.seed(1234)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
minimumFrequency <- 0
DTM <- DocumentTermMatrix(docs, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# number of topics
K <- 7
# set random number generator seed
set.seed(1234)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
DTM
DTM_df <- data.frame(DTM)
apply(DTM,1,FUN=sum)!=0
index_nonzero <- apply(DTM,1,FUN=sum)!=0
DTM <- DTM[apply(DTM,1,FUN=sum)!=0,]
# number of topics
K <- 7
# set random number generator seed
set.seed(1234)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
minimumFrequency <- 10
DTM <- DocumentTermMatrix(docs, control = list(bounds = list(global = c(minimumFrequency, Inf))))
DTM <- DTM[apply(DTM,1,FUN=sum)!=0,]
# store rows that are dropped
index_nonzero <- apply(DTM,1,FUN=sum)!=0
# number of topics
K <- 7
# set random number generator seed
set.seed(1234)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
### FORM TOPICS
tmResult <- posterior(topicModel)
beta <- tmResult$terms
theta <- tmResult$topics
terms(topicModel, 10)
exampleTermData <- terms(topicModel, 10)
top5termsPerTopic <- terms(topicModel, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")
# text_df %>% select(Company, Year)
theta_df <- data.frame(theta)
theta_df$Company <- text_df[index_nonzero]$Company
theta_df$Year <- text_df[index_nonzero]$Year
theta_df$page <- text_df[index_nonzero]$page
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code/2. After removing additional stopwords")
write.csv(theta_df, "topic_distribution_v6_7topics.csv",row.names=F)
text_df
text_df[index_nonzero]
data.frame(index_nonzero)
data.frame(index_nonzero,header=F)
data.frame(index_nonzero)
data.frame(index_nonzero,header=None)
data.frame(index_nonzero,header=0)
# Compute Document Term Matrix where Word >= minimumFrequency
minimumFrequency <- 10
DTM <- DocumentTermMatrix(docs, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# store rows that are dropped
index_nonzero <- apply(DTM,1,FUN=sum)!=0
DTM <- DTM[index_nonzero,]
# number of topics
K <- 7
# set random number generator seed
set.seed(1234)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
### FORM TOPICS
tmResult <- posterior(topicModel)
beta <- tmResult$terms
theta <- tmResult$topics
terms(topicModel, 10)
exampleTermData <- terms(topicModel, 10)
top5termsPerTopic <- terms(topicModel, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")
theta_df <- data.frame(theta)
data.frame(index_nonzero,header=0)
data.frame(index_nonzero,header=0)$index_nonzero
index_nonzero <- data.frame(index_nonzero,header=0)$index_nonzero
text_df[index_nonzero]
text_df[index_nonzero,]
theta_df$Company <- text_df[index_nonzero,]$Company
theta_df$Year <- text_df[index_nonzero,]$Year
theta_df$page <- text_df[index_nonzero,]$page
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code/2. After removing additional stopwords")
write.csv(theta_df, "topic_distribution_v6_7topics.csv",row.names=F)
for (i in 1:K){
## Topic Wordcloud
topicToViz <- i
top100terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:100]
words <- names(top100terms)
# extract the probabilites of each of the 40 terms
probabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:100]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
# Generate the Word Cloud
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code/2. After removing additional stopwords")
png(paste(i,".png",sep=""), width=480,height=480)
wordcloud(words, probabilities, random.order = FALSE, color = mycolors, rot.per=0)
dev.off()
}
library(tidyverse)
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code")
read.csv("combined dataset.csv")
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code")
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code/")
read.csv("combined dataset.csv")
read.csv("combined dataset.xlsx")
read.csv("combined dataset.csv")
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code/")
dir()
read.csv("combined dataset.csv")
data <- read.csv("combined dataset.csv")
data %>% names()
data$Company <- data$"i..Company"
data %>% names()
data <- read.csv("combined dataset.csv", fileEncoding = "UTF-8-BOM")
data %>% names()
gc()
data <- read.csv("combined dataset.csv", fileEncoding = "UTF-8-BOM")
data %>% names()
data <- read.csv("combined dataset.csv", fileEncoding = "UTF-8-BOM")
data %>% names()
roa_rq2 <- lm(ROA ~ t_Community + t_Resources + t_Customers + t_Employees + t_Methodology + t_Governance + t_Climate, data )
summary(roa_rq2)
data %>% names()
summary(roa_rq3)
roa_rq3 <- lm(ROA ~ flesch + sentiment, data)
summary(roa_rq3)
mv_rq2 <- lm(MV_t4 ~ BV_t + EARN_t + EARN_t * NEG_t + t_Community + t_Resources + t_Customers + t_Employees + t_Methodology + t_Governance + t_Climate, data )
summary(mv_rq2)
mv_rq3 <- lm(MV_t4 ~ BV_t + EARN_t + EARN_t * NEG_t + flesch + sentiment, data)
summary(mv_rq3)
data %>% names()
zs_rq2 <- lm(zmijewski_t ~ t_Community + t_Resources + t_Customers + t_Employees + t_Methodology + t_Governance + t_Climate, data )
summary(zs_rq2)
data$t_Climate
zs_rq3 <- lm(zmijewski_t ~ flesch + sentiment, data)
summary(zs_rq3)
summary(zs_rq2)
summary(zs_rq2)
zs_rq2 <- lm(zmijewski_t ~ t_Community + t_Resources + t_Customers + t_Employees + t_Methodology + t_Governance + t_Climate, data )
summary(zs_rq2)
mv_rq2 <- lm(MV_t4 ~ BV_t + EARN_t + EARN_t * NEG_t + t_Community + t_Resources + t_Customers + t_Employees + t_Methodology + t_Governance + t_Climate, data )
summary(mv_rq2)
summary(lm(zmijewski_t ~ flesch, data))
summary(lm(zmijewski_t ~ sentiment, data))
summary(roa_rq2)
summary(roa_rq3)
summary(roa_rq3)
summary(mv_rq3)
summary(mv_rq2)
summary(zs_rq2)
summary(zs_rq3)
summary(lm(zmijewski_t ~ flesch, data))
summary(lm(zmijewski_t ~ sentiment, data))
zs_rq3 <- lm(zmijewski_t ~ flesch + sentiment, data)
summary(zs_rq3)
roa_comb <- lm(ROA ~ t_Community + t_Resources + t_Customers + t_Employees + t_Methodology + t_Governance + t_Climate + flesch + sentiment, data )
library(tidyverse)
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code/")
data <- read.csv("combined dataset.csv", fileEncoding = "UTF-8-BOM")
data %>% names()
summary(roa_comb)
roa_comb <- lm(ROA ~ t_Community + t_Resources + t_Customers + t_Employees + t_Methodology + t_Governance + t_Climate + flesch + sentiment, data )
summary(roa_comb)
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code/")
data <- read.csv("combined dataset.csv", fileEncoding = "UTF-8-BOM")
data %>% names()
roa_rq2_w <- lm(ROA ~ w_Community + w_Resources + w_Customers + w_Employees + w_Methodology + w_Governance + w_Climate, data )
summary(roa_rq2_w)
roa_rq2_w <- lm(ROA ~ w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate, data )
summary(roa_rq2_w)
mv_rq2_w <- lm(MV_t4 ~ BV_t + EARN_t + EARN_t * NEG_t + w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate, data )
summary(mv_rq2_w)
zs_rq2_w <- lm(zmijewski_t ~ w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate, data )
summary(zs_rq2_w)
mv_rq2_w <- lm(MV_t4 ~ BV_t + EARN_t + EARN_t * NEG_t + w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate, data )
summary(mv_rq2_w)
mv_rq2 <- lm(MV_t4 ~ BV_t + EARN_t + EARN_t * NEG_t + t_Community + t_Resources + t_Customers + t_Employees + t_Methodology + t_Governance + t_Climate, data )
summary(mv_rq2)
mv_rq2 <- lm(MV_t4 ~ BV_t + EARN_t + EARN_t * NEG_t + t_Community + t_Resources + t_Customers + t_Employees + t_Methodology + t_Governance + t_Climate, data )
data %>% names()
roa_rq2 <- lm(ROA ~ t_Community + t_Resources + t_Customers + t_Employees + t_Metholodgy + t_Governance + t_Climate, data )
summary(roa_rq2)
roa_rq2_w <- lm(ROA ~ w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate, data )
summary(roa_rq2_w)
roa_rq3 <- lm(ROA ~ flesch + sentiment, data)
summary(roa_rq3)
mv_rq2 <- lm(MV_t4 ~ BV_t + EARN_t + EARN_t * NEG_t + t_Community + t_Resources + t_Customers + t_Employees + t_Metholodgy + t_Governance + t_Climate, data )
summary(mv_rq2)
mv_rq2_w <- lm(MV_t4 ~ BV_t + EARN_t + EARN_t * NEG_t + w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate, data )
summary(mv_rq2_w)
mv_rq2 <- lm(MV_t4 ~ BV_t + EARN_t + EARN_t * NEG_t + t_Community + t_Resources + t_Customers + t_Employees + t_Metholodgy + t_Governance + t_Climate, data )
summary(mv_rq2)
mv_rq2_w <- lm(MV_t4 ~ BV_t + EARN_t + EARN_t * NEG_t + w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate, data )
summary(mv_rq2_w)
mv_rq3 <- lm(MV_t4 ~ BV_t + EARN_t + EARN_t * NEG_t + flesch + sentiment, data)
summary(mv_rq3)
zs_rq2 <- lm(zmijewski_t ~ t_Community + t_Resources + t_Customers + t_Employees + t_Metholodgy + t_Governance + t_Climate, data )
summary(zs_rq2)
zs_rq2_w <- lm(zmijewski_t ~ w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate, data )
summary(zs_rq2_w)
zs_rq2_w <- lm(zmijewski_t ~ w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate, data )
summary(zs_rq2_w)
zs_rq2 <- lm(zmijewski_t ~ t_Community + t_Resources + t_Customers + t_Employees + t_Metholodgy + t_Governance + t_Climate, data )
summary(zs_rq2)
zs_rq2_w <- lm(zmijewski_t ~ w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate, data )
summary(zs_rq2_w)
zs_rq2 <- lm(zmijewski_t ~ t_Community + t_Resources + t_Customers + t_Employees + t_Metholodgy + t_Governance + t_Climate, data )
summary(zs_rq2)
zs_rq2_w <- lm(zmijewski_t ~ w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate, data )
summary(zs_rq2_w)
zs_rq3 <- lm(zmijewski_t ~ flesch + sentiment, data)
summary(zs_rq3)
roa_comb <- lm(ROA ~ w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate + flesch + sentiment, data )
summary(roa_comb)
roa_rq2_w <- lm(ROA ~ w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate, data )
summary(roa_rq2_w)
roa_rq2 <- lm(ROA ~ t_Community + t_Resources + t_Customers + t_Employees + t_Metholodgy + t_Governance + t_Climate, data )
summary(roa_rq2)
zs_comb <- lm(zmijewski_t ~ w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate + flesch + sentiment, data )
summary(zs_comb)
roa_rq2_w <- lm(ROA ~ w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate, data )
summary(roa_rq2_w)
roa_rq2 <- lm(ROA ~ t_Community + t_Resources + t_Customers + t_Employees + t_Metholodgy + t_Governance + t_Climate, data )
summary(roa_rq2)
roa_rq2_w <- lm(ROA ~ w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate, data )
summary(roa_rq2_w)
roa_rq2_w <- lm(ROA ~ w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate, data )
summary(roa_rq2_w)
roa_rq3 <- lm(ROA ~ flesch + sentiment, data)
summary(roa_rq3)
roa_comb <- lm(ROA ~ w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate + flesch + sentiment, data )
summary(roa_comb)
mv_rq2_w <- lm(MV_t4 ~ BV_t + EARN_t + EARN_t * NEG_t + w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate, data )
summary(mv_rq2_w)
mv_rq2_w <- lm(MV_t4 ~ BV_t + EARN_t + EARN_t * NEG_t + w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate, data )
summary(mv_rq2_w)
mv_rq3 <- lm(MV_t4 ~ BV_t + EARN_t + EARN_t * NEG_t + flesch + sentiment, data)
summary(mv_rq3)
mv_comb <- lm(MV_t4 ~ BV_t + EARN_t + EARN_t * NEG_t + w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate + flesch + sentiment, data )
summary(mv_comb)
zs_rq2_w <- lm(zmijewski_t ~ w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate, data )
summary(zs_rq2_w)
zs_rq3 <- lm(zmijewski_t ~ flesch + sentiment, data)
summary(zs_rq3)
zs_comb <- lm(zmijewski_t ~ w_Community + w_Resources + w_Customers + w_Employees + w_Metholodgy + w_Governance + w_Climate + flesch + sentiment, data )
summary(zs_comb)
cor(sentiment, earn_t)
cor(data$sentiment, data$earn_t)
cor(data$sentiment, data$EARN_t)
cor(data$flesch, data$EARN_t)
cor(data)
cor(data %>% select(-c(Company,year)))
cor(data %>% select(-c(Company,Year)))
cor(data %>% select(-c(Company,Year)))
library(corrplot)
corrplot(data %>% select(-c(Company,Year)), type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45)
install.packages("corrplot")
library(corrplot)
corrplot(data %>% select(-c(Company,Year)), type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45)
res <- cor(data %>% select(-c(Company,Year)))
round(res, 2)
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45)
res <- cor(data %>% select(-c(Company,Year),subset('t_*')))
res <- cor(data %>% select(-c(Company,Year,t_Community,t_Resources,t_Customers,t_Employees,t_Metholodgy,t_Governance,t_Climate)))
round(res, 2)
library(corrplot)
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45)
