remove_hyphens = TRUE,
min_sentence_length = 1,
max_sentence_length = 10000,
intermediate = FALSE
)$Flesch
if (i == 1) {
flesch_scores <- c(flesch_temp)
}
else {
flesch_scores <- c(flesch_scores, flesch_temp)
}
}
is_corpus_text(docs[1])
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
text_df$text <- tm_map(text_df$text, toSpace, "\n")
text_df$text <- str_replace_all(text_df$text, "\n", " ")
text_df$text <- str_replace_all(text_df$text, "/", " ")
text_df$text <- str_replace_all(text_df$text, "@", " ")
text_df$text <- str_replace_all(text_df$text, "\\|", " ")
text_df[1,]$text
text_df$text <- str_replace_all(text_df$text, "•", " ")
text_df[1,]$text
text_df$text <- str_replace_all(text_df$text, "•", " ")
for (i in 1:nrow(text_df)){
flesch_temp <- textstat_readability(
text_df[i,]$text,
measure = "Flesch",
remove_hyphens = TRUE,
min_sentence_length = 1,
max_sentence_length = 10000,
intermediate = FALSE
)$Flesch
if (i == 1) {
flesch_scores <- c(flesch_temp)
}
else {
flesch_scores <- c(flesch_scores, flesch_temp)
}
}
text_df$flesch <- flesch_scores
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code/3. Readability")
write.csv(text_df %>% select(Company, Year, flesch), "readability.csv",row.names = F)
summary(flesch_scores)
get_sentiments("nrc")
library(stringr)
library(janeaustenr)
library(dplyr)
library(stringr)
tidy_books <- austen_books() %>%
group_by(book)
tidy_books
austen_books()
ungroup()
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(
linenumber = row_number(),
chapter = cumsum(str_detect(text,
regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup()
tidy_books
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(
linenumber = row_number(),
chapter = cumsum(str_detect(text,
regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
tidy_books
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE)
text_df
# Reading in Text
for (i in 1:nrow(reports)){
if (i == 1){
text <- pdftools::pdf_text(reports$DocumentName[i])
text_joined <- paste(text, collapse = ' ')
# Initiate the first dataframe
text_df <- tibble(text = text_joined)
text_df$Company <- reports$Company[i]
text_df$Year <- reports$Year[i]
}
else{
text <- pdftools::pdf_text(reports$DocumentName[i])
text_joined <- paste(text, collapse = ' ')
# Create a temporary dataframe
text_df_temp <- tibble(text = text_joined)
text_df_temp$Company <- reports$Company[i]
text_df_temp$Year <- reports$Year[i]
# Append to the original dataframe
text_df <- rbind(text_df,text_df_temp)
}
}
## Text Cleaning
text_df$text <- str_replace_all(text_df$text, "\n", " ")
text_df$text <- str_replace_all(text_df$text, "/", " ")
text_df$text <- str_replace_all(text_df$text, "@", " ")
text_df$text <- str_replace_all(text_df$text, "\\|", " ")
text_df$text <- str_replace_all(text_df$text, "•", " ")
setwd("C:/Users/Jeremy Chia/Documents/GitHub/NLP-SustainabilityReports-FinancialPerformance/Sustainability Reports or Annual Reports")
# Reading in Text
for (i in 1:nrow(reports)){
if (i == 1){
text <- pdftools::pdf_text(reports$DocumentName[i])
text_joined <- paste(text, collapse = ' ')
# Initiate the first dataframe
text_df <- tibble(text = text_joined)
text_df$Company <- reports$Company[i]
text_df$Year <- reports$Year[i]
}
else{
text <- pdftools::pdf_text(reports$DocumentName[i])
text_joined <- paste(text, collapse = ' ')
# Create a temporary dataframe
text_df_temp <- tibble(text = text_joined)
text_df_temp$Company <- reports$Company[i]
text_df_temp$Year <- reports$Year[i]
# Append to the original dataframe
text_df <- rbind(text_df,text_df_temp)
}
}
## Text Cleaning
text_df$text <- str_replace_all(text_df$text, "\n", " ")
text_df$text <- str_replace_all(text_df$text, "/", " ")
text_df$text <- str_replace_all(text_df$text, "@", " ")
text_df$text <- str_replace_all(text_df$text, "\\|", " ")
text_df$text <- str_replace_all(text_df$text, "•", " ")
text_tokenised <- text_df %>%
unnest_tokens (word, text)
text_tokenised %>% head()
# Processing Text of Sustainability Reports
library(pdftools)
library(dplyr)
library(tidytext)
library(tm)
library(textstem)
library(stringr)
library(tidyverse)
# Initiating work directory
setwd("C:/Users/Jeremy Chia/Documents/GitHub/NLP-SustainabilityReports-FinancialPerformance/")
# Defining variables
reports <- read.csv("sustainability_reports_sgx_sti.csv")
setwd("C:/Users/Jeremy Chia/Documents/GitHub/NLP-SustainabilityReports-FinancialPerformance/Sustainability Reports or Annual Reports")
# Reading in Text
for (i in 1:nrow(reports)){
if (i == 1){
text <- pdftools::pdf_text(reports$DocumentName[i])
# Initiate the first dataframe
text_df <- tibble(page = 1:length(text), text = text)
text_df$Company <- reports$Company[i]
text_df$Year <- reports$Year[i]
}
else{
text <- pdftools::pdf_text(reports$DocumentName[i])
# Create a temporary dataframe
text_df_temp <- tibble(page = 1:length(text), text = text)
text_df_temp$Company <- reports$Company[i]
text_df_temp$Year <- reports$Year[i]
# Append to the original dataframe
text_df <- rbind(text_df,text_df_temp)
}
}
## Text Cleaning
text_df$text <- str_replace_all(text_df$text, "\n", " ")
text_df$text <- str_replace_all(text_df$text, "/", " ")
text_df$text <- str_replace_all(text_df$text, "@", " ")
text_df$text <- str_replace_all(text_df$text, "\\|", " ")
text_df$text <- str_replace_all(text_df$text, "•", " ")
## Unnesting Tokens
text_tokenised <- text_df %>%
unnest_tokens (word, text)
text_tokenised %>%
group_by(Company,Year) %>%
count()
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code")
write.csv(text_tokenised %>%
group_by(Company,Year) %>%
count(), "number_words.csv", row.names = F)
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code/4. Sentiment Analysis")
get_sentiments("afinn")
get_sentiments("nrc")
get_sentiments("bing")
get_sentiments("jockers")
get_sentiments("loughran")
get_sentiments("loughran")
library(lexicon)
key_sentiment_jockers
text_tokenised %>%
inner_join(get_sentiments("afinn")) %>% head()
text_tokenised %>%
inner_join(get_sentiments("afinn")) %>%
group_by(page) %>%
summarise(sentiment = sum(value))
text_tokenised %>%
inner_join(get_sentiments("afinn")) %>%
group_by(Company, year, page) %>%
summarise(sentiment = sum(value)) %>%
mutate(method = "AFINN")
text_tokenised %>%
inner_join(get_sentiments("afinn")) %>%
group_by(Company, year, page) %>%
summarise(sentiment = sum(value))
text_tokenised %>%
inner_join(get_sentiments("afinn")) %>%
group_by(Company, Year, page) %>%
summarise(sentiment = sum(value))
text_tokenised %>%
inner_join(get_sentiments("afinn")) %>%
group_by(Company, Year, page) %>%
summarise(sentiment = sum(value)) %>%
mutate(method = "AFINN")
text_tokenised %>%
inner_join(get_sentiments("afinn")) %>%
group_by(Company, Year, page) %>%
summarise(sentiment = sum(value)) %>%
mutate(method = "AFINN")
get_sentiments("afinn")
summarise(text_tokenised %>%
inner_join(get_sentiments("afinn")) %>%
group_by(Company, Year, page) %>%
summarise(sentiment = sum(value)) %>%
mutate(method = "AFINN")$sentiment)
afinn <- text_tokenised %>%
inner_join(get_sentiments("afinn")) %>%
group_by(Company, Year, page) %>%
summarise(sentiment = sum(value)) %>%
mutate(method = "AFINN")
afinn
summary(afinn$sentiment)
afinn <- text_tokenised %>%
inner_join(get_sentiments("afinn")) %>%
group_by(Company, Year) %>%
summarise(sentiment = sum(value)) %>%
mutate(method = "AFINN")
summarise((afinn %>% group_by(Company, year) %>% summarise(sentiment = sum(sentiment)))$sentiment)
summarise((afinn %>% group_by(Company, Year) %>% summarise(sentiment = sum(sentiment)))$sentiment)
afinn %>% group_by(Company, Year)
(afinn %>% group_by(Company, Year))$sentiment
summarise((afinn %>% group_by(Company, Year))$sentiment)
summary((afinn %>% group_by(Company, Year))$sentiment)
nrc <- token_tokenised %>%
inner_join(get_sentiments("nrc"))
nrc <- text_tokenised %>%
inner_join(get_sentiments("nrc"))
nrc
nrc <- text_tokenised %>%
inner_join(get_sentiments("nrc")) %>%
filter(sentiment %in% c("positive","negative"))
nrc <- text_tokenised %>%
inner_join(get_sentiments("nrc")) %>%
filter(sentiment %in% c("positive","negative")) %>%
mutate(method = "NRC")
nrc <- text_tokenised %>%
inner_join(get_sentiments("nrc")) %>%
filter(sentiment %in% c("positive","negative")) %>%
mutate(method = "NRC") %>%
pivot_wider(names_from = sentiment,
values_from = n,
values_fill = 0) %>%
mutate(sentiment = positive - negative)
nrc <- text_tokenised %>%
inner_join(get_sentiments("nrc")) %>%
filter(sentiment %in% c("positive","negative")) %>%
mutate(method = "NRC") %>%
count()
nrc
nrc <- text_tokenised %>%
inner_join(get_sentiments("nrc")) %>%
filter(sentiment %in% c("positive","negative")) %>%
mutate(method = "NRC")
text_tokenised %>%
inner_join(get_sentiments("nrc")) %>%
filter(sentiment %in% c("positive","negative")) %>%
mutate(method = "NRC")
text_tokenised %>%
inner_join(get_sentiments("nrc")) %>%
filter(sentiment %in% c("positive","negative")) %>%
mutate(method = "NRC") %>%
count(Company, Year, sentiment)
text_tokenised %>%
inner_join(get_sentiments("nrc")) %>%
filter(sentiment %in% c("positive","negative")) %>%
mutate(method = "NRC") %>%
count(Company, Year, sentiment) %>%
pivot_wider(names_from = sentiment,
values_from = n,
values_fill = 0)
pivot_wider(names_from = sentiment,
values_from = n,
values_fill = 0) %>%
mutate(sentiment = positive - negative)
nrc <- text_tokenised %>%
inner_join(get_sentiments("nrc")) %>%
filter(sentiment %in% c("positive","negative")) %>%
mutate(method = "NRC") %>%
count(Company, Year, sentiment) %>%
pivot_wider(names_from = sentiment,
values_from = n,
values_fill = 0) %>%
mutate(sentiment = positive - negative)
nrc
nrc <- text_tokenised %>%
inner_join(get_sentiments("nrc")) %>%
filter(sentiment %in% c("positive","negative")) %>%
mutate(method = "NRC") %>%
count(Company, Year, page, sentiment) %>%
pivot_wider(names_from = sentiment,
values_from = n,
values_fill = 0) %>%
mutate(sentiment = positive - negative)
nrc
summary((nrc %>% group_by(Company, Year))$sentiment)
bing <- text_tokenised %>%
inner_join(get_sentiments("bing")) %>%
filter(sentiment %in% c("positive","negative")) %>%
mutate(method = "Bing et al.") %>%
count(Company, Year, page, sentiment) %>%
pivot_wider(names_from = sentiment,
values_from = n,
values_fill = 0) %>%
mutate(sentiment = positive - negative)
summary((bing %>% group_by(Company, Year))$sentiment)
loughran <- text_tokenised %>%
inner_join(get_sentiments("loughran")) %>%
count(sentiment)
text_tokenised %>%
inner_join(get_sentiments("loughran")) %>%
count(sentiment)
loughran <- text_tokenised %>%
inner_join(get_sentiments("loughran")) %>%
filter(sentiment %in% c("positive","negative")) %>%
mutate(method = "Loughran") %>%
count(Company, Year, page, sentiment) %>%
pivot_wider(names_from = sentiment,
values_from = n,
values_fill = 0) %>%
mutate(sentiment = positive - negative)
summary((loughran %>% group_by(Company, Year))$sentiment)
text_tokenised %>%
inner_join(key_sentiment_jockers)
jockers <- text_tokenised %>%
inner_join(key_sentiment_jockers) %>%
group_by(Company, Year, page) %>%
summarise(sentiment = sum(value)) %>%
mutate(method = "Jockers")
summary((jockers %>% group_by(Company, Year))$sentiment)
# Processing Text of Sustainability Reports
library(pdftools)
library(dplyr)
library(tidytext)
library(tm)
library(textstem)
library(topicmodels)
library(lda)
library(ldatuning)
library(ggplot2)
library(wordcloud)
library(pals)
library(SnowballC)
library(stringr)
# Initiating work directory
setwd("C:/Users/Jeremy Chia/Documents/GitHub/NLP-SustainabilityReports-FinancialPerformance/")
# Defining variables
reports <- read.csv("sustainability_reports_sgx_sti.csv")
setwd("C:/Users/Jeremy Chia/Documents/GitHub/NLP-SustainabilityReports-FinancialPerformance/Sustainability Reports or Annual Reports")
# Reading in Text
for (i in 1:nrow(reports)){
if (i == 1){
text <- pdftools::pdf_text(reports$DocumentName[i])
# Initiate the first dataframe
text_df <- tibble(page = 1:length(text), text = text)
text_df$Company <- reports$Company[i]
text_df$Year <- reports$Year[i]
}
else{
text <- pdftools::pdf_text(reports$DocumentName[i])
# Create a temporary dataframe
text_df_temp <- tibble(page = 1:length(text), text = text)
text_df_temp$Company <- reports$Company[i]
text_df_temp$Year <- reports$Year[i]
# Append to the original dataframe
text_df <- rbind(text_df,text_df_temp)
}
}
# Summarising Results
number_pages <- text_df %>%
group_by(Company, Year) %>%
count()
number_pages %>%
mutate(
Year = as.character(Year)
) %>%
ggplot(aes(x=Year,y=n)) +
geom_boxplot(aes(x=Year), fill = "grey", alpha = 0.2) +
geom_jitter(width = 0.1, alpha = 0.5) +
theme_minimal() +
ggtitle("Number of Pages by Year") +
ylab("Number of Pages") +
xlab("Disclosure for Year Ended In")
theme(legend.position="none") +
scale_fill_brewer(palette="Blues")
setwd("C:/Users/Jeremy Chia/Documents/GitHub/NLP-SustainabilityReports-FinancialPerformance/")
write.csv(number_pages, "number_of_pages.csv", row.names=F)
# Converting to a Corpus
docs <- Corpus(VectorSource(text_df$text))
# Text Cleaning
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "\n")
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# docs <- tm_map(docs, toSpace, "*")
docs <- tm_map(docs, toSpace, "[^[:alnum:] ]")
# setwd("C:/Users/Jeremy Chia/Documents/GitHub/NLP-SustainabilityReports-FinancialPerformance/")
# characters_remove <- read.csv("characters.csv")
#
# for (i in 1:nrow(characters_remove)){
#   docs <- tm_map(docs, toSpace, characters_remove$X.[i])
#   print(i)
# }
# Text Cleaning
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove english (SMART) common stopwords
docs <- tm_map(docs, removeWords, stopwords("SMART"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("reporting","period","report","fy")) # reporting words
## Remove additional stopwords as identified
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code/1. Before removing additional stopwords")
to_remove <- as.vector(read.csv("2. to_remove.csv",header=T)[[1]])
docs <- tm_map(docs, removeWords, to_remove) # remove company, country, irrelevant words
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text lemmization
docs <- lemmatize_words(docs, dictionary = lexicon::hash_lemmas)
#####
# Compute Document Term Matrix where Word >= minimumFrequency
minimumFrequency <- 10
DTM <- DocumentTermMatrix(docs, control = list(bounds = list(global = c(minimumFrequency, Inf))))
DTM <- DTM[apply(DTM,1,FUN=sum)!=0,]
# create models with different number of topics
result <- ldatuning::FindTopicsNumber(
DTM,
topics = seq(from = 2, to = 50, by = 1),
metrics = c("CaoJuan2009",  "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
verbose = TRUE
)
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code/2. After removing additional stopwords")
write.csv(result,"metrics_v3.csv")
FindTopicsNumber_plot(result)
# number of topics
K <- 11
# set random number generator seed
set.seed(1234)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
topics <- tidy(topicModel, matrix = "beta")
# get the top ten terms for each topic
top_terms <- topics  %>% # take the topics data frame and..
group_by(topic) %>% # treat each topic as a different group
top_n(10, beta) %>% # get the top 10 most informative words
ungroup() %>% # ungroup
arrange(topic, -beta) # arrange words in descending informativeness
### GRAPH OUT TOP TERMS
top_terms %>% # take the top terms
mutate(term = reorder(term, beta)) %>% # sort terms by beta value
ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
geom_col(show.legend = FALSE) + # as a bar plot
facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
labs(x = NULL, y = "Beta") + # no x label, change y label
coord_flip() # turn bars sideways
write.csv(top_terms,"top_words_v3.csv",fileEncoding = "utf8")
theme(legend.position="none") +
scale_fill_brewer(palette="Blues")
library(ggplot2)
library(pals)
library(SnowballC)
theme(legend.position="none") +
scale_fill_brewer(palette="Blues")
text_tokenised %>%
group_by(Company,Year) %>%
count() %>%
mutate(
Year = as.character(Year)
) %>%
ggplot(aes(x=Year,y=n)) +
geom_boxplot(aes(x=Year), fill = "grey", alpha = 0.2) +
geom_jitter(width = 0.1, alpha = 0.5) +
theme_minimal() +
ggtitle("Number of Words by Year") +
ylab("Number of Words") +
xlab("Disclosure for Year Ended In")
theme(legend.position="none")
