essing Text of Sustainability Reports
library(pdftools)
library(dplyr)
library(tidytext)
library(tm)
library(textstem)
library(topicmodels)
library(lda)
library(ldatuning)
library(ggplot2)
library(wordcloud)
library(pals)
library(SnowballC)
library(stringr)
# Initiating work directory
setwd("C:/Users/Jeremy Chia/Documents/GitHub/NLP-SustainabilityReports-FinancialPerformance/")
# Defining variables
reports <- read.csv("sustainability_reports_sgx_sti.csv")
setwd("C:/Users/Jeremy Chia/Documents/GitHub/NLP-SustainabilityReports-FinancialPerformance/Sustainability Reports or Annual Reports")
# Reading in Text
for (i in 1:nrow(reports)){
if (i == 1){
text <- pdftools::pdf_text(reports$DocumentName[i])
text_joined <- paste(text, collapse = ' ')
# Initiate the first dataframe
text_df <- tibble(text = text_joined)
text_df$Company <- reports$Company[i]
text_df$Year <- reports$Year[i]
}
else{
text <- pdftools::pdf_text(reports$DocumentName[i])
text_joined <- paste(text, collapse = ' ')
# Create a temporary dataframe
text_df_temp <- tibble(text = text_joined)
text_df_temp$Company <- reports$Company[i]
text_df_temp$Year <- reports$Year[i]
# Append to the original dataframe
text_df <- rbind(text_df,text_df_temp)
}
}
# Converting to a Corpus
docs <- Corpus(VectorSource(text_df$text))
# Text Cleaning
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "\n")
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "[^[:alnum:] ]")
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, stopwords("SMART"))
docs <- tm_map(docs, removeWords, c("reporting","period","report","fy")) # reporting words
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code/1. Before removing additional stopwords")
to_remove <- as.vector(read.csv("2. to_remove.csv",header=T)[[1]])
docs <- tm_map(docs, removeWords, to_remove) # remove company, country, irrelevant words
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- lemmatize_words(docs, dictionary = lexicon::hash_lemmas)
# Compute Document Term Matrix where Word >= minimumFrequency
minimumFrequency <- 10
DTM <- DocumentTermMatrix(docs, control = list(bounds = list(global = c(minimumFrequency, Inf))))
DTM <- DTM[apply(DTM,1,FUN=sum)!=0,]
# number of topics
K <- 11
# set random number generator seed
set.seed(1234)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
tmResult <- posterior(topicModel)
beta <- tmResult$terms
theta <- tmResult$topics
terms(topicModel, 10)
top5termsPerTopic <- terms(topicModel, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")
# re-rank top topic terms for topic names
topicNames <- apply(lda::top.topic.words(beta, 5, by.score = T), 2, paste, collapse = " ")
# What are the most probable topics in the entire collection?
topicProportions <- colSums(theta) / nDocs(DTM)  # mean probablities over all paragraphs
names(topicProportions) <- topicNames     # assign the topic names we created before
sort(topicProportions, decreasing = TRUE) # show summed proportions in decreased order
# We count how often a topic appears as a primary topic within a paragraph This method is also called Rank-1.
countsOfPrimaryTopics <- rep(0, K)
names(countsOfPrimaryTopics) <- topicNames
for (i in 1:nDocs(DTM)) {
topicsPerDoc <- theta[i, ] # select topic distribution for document i
# get first element position from ordered list
primaryTopic <- order(topicsPerDoc, decreasing = TRUE)[1]
countsOfPrimaryTopics[primaryTopic] <- countsOfPrimaryTopics[primaryTopic] + 1
}
sort(countsOfPrimaryTopics, decreasing = TRUE)
so <- sort(countsOfPrimaryTopics, decreasing = TRUE)
paste(so, ":", names(so))
## Remove additional stopwords as identified
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code/1. Before removing additional stopwords")
to_remove <- as.vector(read.csv("2. to_remove.csv",header=T)[[1]])
docs <- tm_map(docs, removeWords, to_remove) # remove company, country, irrelevant words
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
DTM <- DocumentTermMatrix(docs, control = list(bounds = list(global = c(minimumFrequency, Inf))))
DTM <- DTM[apply(DTM,1,FUN=sum)!=0,]
# number of topics
K <- 11
# set random number generator seed
set.seed(1234)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
tmResult <- posterior(topicModel)
beta <- tmResult$terms
theta <- tmResult$topics
terms(topicModel, 10)
exampleTermData <- terms(topicModel, 10)
top5termsPerTopic <- terms(topicModel, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")
topicNames
# get topic proportions form example documents
topicProportionExamples <- theta[exampleIds,]
# get topic proportions form example documents
topicProportionExamples <- theta[1:3,]
theta[1:3,]
colnames(topicProportionExamples) <- topicNames
vizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = "topic", id.vars = "document")
library(quanteda)
vizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = "topic", id.vars = "document")
library(reshape2)
vizDataFrame <- reshape2::melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = "topic", id.vars = "document")
vizDataFrame <- reshape2::melt(cbind(data.frame(topicProportionExamples), document = factor(1:3)), variable.name = "topic", id.vars = "document")
ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") +
geom_bar(stat="identity") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
coord_flip() +
facet_wrap(~ document, ncol = N)
ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") +
geom_bar(stat="identity") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
coord_flip() +
facet_wrap(~ document, ncol = 3)
theta
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code/1. Before removing additional stopwords")
write.csv(theta, "topic_distribution.csv",row.names=F)
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code/2. After removing additional stopwords")
write.csv(theta, "topic_distribution.csv",row.names=F)
text_df %>% head()
text_df %>% select(Company, Year)
theta$Company <- text_df$Company
theta$Year <- text_df$Year
theta
theta %>% head()
theta$1
theta <- tmResult$topics
theta %>% head()
theta$1
data.frame(theta)
data.frame(theta) %>% head()
theta_df <- data.frame(theta)
theta_df$Company <- text_df$Company
theta_df$Year <- text_df$Year
theta_df %>% head()
write.csv(theta_df, "topic_distribution.csv",row.names=F)
# What are the most probable topics in the entire collection?
topicProportions <- colSums(theta) / nDocs(DTM)  # mean probablities over all paragraphs
names(topicProportions) <- topicNames     # assign the topic names we created before
sort(topicProportions, decreasing = TRUE) # show summed proportions in decreased order
# re-rank top topic terms for topic names
topicNames <- apply(lda::top.topic.words(beta, 5, by.score = T), 2, paste, collapse = " ")
topicNames
# get the top ten terms for each topic
top_terms <- topics  %>% # take the topics data frame and..
group_by(topic) %>% # treat each topic as a different group
top_n(10, beta) %>% # get the top 10 most informative words
ungroup() %>% # ungroup
arrange(topic, -beta) # arrange words in descending informativeness
### GRAPH OUT TOP TERMS
top_terms %>% # take the top terms
mutate(term = reorder(term, beta)) %>% # sort terms by beta value
ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
geom_col(show.legend = FALSE) + # as a bar plot
facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
labs(x = NULL, y = "Beta") + # no x label, change y label
coord_flip() # turn bars sideways
write.csv(top_terms,"top_words_v4.csv",fileEncoding = "utf8")
topics <- tidy(topicModel, matrix = "beta")
# get the top ten terms for each topic
top_terms <- topics  %>% # take the topics data frame and..
group_by(topic) %>% # treat each topic as a different group
top_n(10, beta) %>% # get the top 10 most informative words
ungroup() %>% # ungroup
arrange(topic, -beta) # arrange words in descending informativeness
### GRAPH OUT TOP TERMS
top_terms %>% # take the top terms
mutate(term = reorder(term, beta)) %>% # sort terms by beta value
ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
geom_col(show.legend = FALSE) + # as a bar plot
facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
labs(x = NULL, y = "Beta") + # no x label, change y label
coord_flip() # turn bars sideways
write.csv(top_terms,"top_words_v4.csv",fileEncoding = "utf8")
# We count how often a topic appears as a primary topic within a paragraph This method is also called Rank-1.
countsOfPrimaryTopics <- rep(0, K)
names(countsOfPrimaryTopics) <- topicNames
for (i in 1:nDocs(DTM)) {
topicsPerDoc <- theta[i, ] # select topic distribution for document i
# get first element position from ordered list
primaryTopic <- order(topicsPerDoc, decreasing = TRUE)[1]
countsOfPrimaryTopics[primaryTopic] <- countsOfPrimaryTopics[primaryTopic] + 1
}
sort(countsOfPrimaryTopics, decreasing = TRUE)
so <- sort(countsOfPrimaryTopics, decreasing = TRUE)
paste(so, ":", names(so))
for (i in 1:K){
## Topic Wordcloud
topicToViz <- i
top100terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:100]
words <- names(top100terms)
# extract the probabilites of each of the 40 terms
probabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:100]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
# Generate the Word Cloud
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code/2. After removing additional stopwords")
png(paste(i,".png",sep=""), width=480,height=480)
wordcloud(words, probabilities, random.order = FALSE, color = mycolors, rot.per=0)
dev.off()
}
topicNames
# number of topics
K <- 18
# set random number generator seed
set.seed(1234)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
tmResult <- posterior(topicModel)
beta <- tmResult$terms
theta <- tmResult$topics
terms(topicModel, 10)
# create models with different number of topics
result <- ldatuning::FindTopicsNumber(
DTM,
topics = seq(from = 2, to = 50, by = 1),
metrics = c("CaoJuan2009",  "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
verbose = TRUE
)
setwd("C:/Users/Jeremy Chia/Desktop/2022/Research/Code/2. After removing additional stopwords")
write.csv(result,"metrics_v4.csv")
FindTopicsNumber_plot(result)
