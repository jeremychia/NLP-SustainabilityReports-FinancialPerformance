# Text Cleaning
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "\n")
docs <- tm_map(docs, toSpace, "/")
library(pdftools)
library(dplyr)
library(tidytext)
library(tm)
library(textstem)
library(topicmodels)
library(lda)
library(ldatuning)
library(ggplot2)
library(wordcloud)
library(pals)
library(SnowballC)
library(stringr)
# Initiating work directory
setwd("C:/Users/Jeremy Chia/Documents/GitHub/NLP-SustainabilityReports-FinancialPerformance/")
# Defining variables
reports <- read.csv("sustainability_reports_sgx_sti.csv")
setwd("C:/Users/Jeremy Chia/Documents/GitHub/NLP-SustainabilityReports-FinancialPerformance/Sustainability Reports or Annual Reports")
# Test variables
reports$Company[1]
reports$Year[1]
# Reading in Text
for (i in 1:5){ #nrow(reports)
if (i == 1){
text <- pdftools::pdf_text(reports$DocumentName[i])
# Initiate the first dataframe
text_df <- tibble(page = 1:length(text), text = text)
text_df$Company <- reports$Company[i]
text_df$Year <- reports$Year[i]
}
else{
text <- pdftools::pdf_text(reports$DocumentName[i])
# Create a temporary dataframe
text_df_temp <- tibble(page = 1:length(text), text = text)
text_df_temp$Company <- reports$Company[i]
text_df_temp$Year <- reports$Year[i]
# Append to the original dataframe
text_df <- rbind(text_df,text_df_temp)
}
}
# Summarising Results
text_df %>%
group_by(Company, Year) %>%
count()
# Converting to a Corpus
docs <- Corpus(VectorSource(text_df$text))
# Text Cleaning
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "\n")
docs[[1]]$content
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "*")
docs[[1]]$content
# Converting to a Corpus
docs <- Corpus(VectorSource(text_df$text))
# Text Cleaning
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "\n")
docs[[1]]$content
docs <- tm_map(docs, toSpace, "/")
docs[[1]]$content
docs <- tm_map(docs, toSpace, "@")
docs[[1]]$content
docs <- tm_map(docs, toSpace, "\\|")
docs[[1]]$content
docs <- tm_map(docs, toSpace, "*")
docs[[1]]$content
docs <- Corpus(VectorSource(text_df$text))
# Text Cleaning
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "\n")
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# docs <- tm_map(docs, toSpace, "*")
docs[[1]]$content
# docs <- tm_map(docs, toSpace, "*")
docs <- tm_map(docs, toSpace, "[^[:alnum:] ]")
docs[[1]]$content
# Text Cleaning
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
docs[[1]]$content
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("reporting","period","report"))
docs[[1]]$content
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
docs[[1]]$content# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
docs[[1]]$content
docs[[1]]$content
# Text lemmization
docs <- lemmatize_words(docs, dictionary = lexicon::hash_lemmas)
docs[[1]]$content
# Reading in Text
for (i in 1:nrow(reports)){
if (i == 1){
text <- pdftools::pdf_text(reports$DocumentName[i])
# Initiate the first dataframe
text_df <- tibble(page = 1:length(text), text = text)
text_df$Company <- reports$Company[i]
text_df$Year <- reports$Year[i]
}
else{
text <- pdftools::pdf_text(reports$DocumentName[i])
# Create a temporary dataframe
text_df_temp <- tibble(page = 1:length(text), text = text)
text_df_temp$Company <- reports$Company[i]
text_df_temp$Year <- reports$Year[i]
# Append to the original dataframe
text_df <- rbind(text_df,text_df_temp)
}
}
# Summarising Results
text_df %>%
group_by(Company, Year) %>%
count()
# Converting to a Corpus
docs <- Corpus(VectorSource(text_df$text))
# Text Cleaning
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "\n")
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# docs <- tm_map(docs, toSpace, "*")
docs <- tm_map(docs, toSpace, "[^[:alnum:] ]")
for (i in 1:9811){
print(docs[[i]]$content)
}
# Text Cleaning
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("reporting","period","report"))
docs[[100]]$content
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("reporting","period","report","fy"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
docs[[100]]$content
# Text lemmization
docs <- lemmatize_words(docs, dictionary = lexicon::hash_lemmas)
# number of topics
K <- 13
# set random number generator seed
set.seed(1234)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
topics <- tidy(topicModel, matrix = "beta")
# get the top ten terms for each topic
top_terms <- topics  %>% # take the topics data frame and..
group_by(topic) %>% # treat each topic as a different group
top_n(10, beta) %>% # get the top 10 most informative words
ungroup() %>% # ungroup
arrange(topic, -beta) # arrange words in descending informativeness
top_terms
# Compute Document Term Matrix where Word >= minimumFrequency
minimumFrequency <- 10
DTM <- DocumentTermMatrix(docs, control = list(bounds = list(global = c(minimumFrequency, Inf))))
DTM <- DTM[apply(DTM,1,FUN=sum)!=0,]
# number of topics
K <- 13
# set random number generator seed
set.seed(1234)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
topics <- tidy(topicModel, matrix = "beta")
# get the top ten terms for each topic
top_terms <- topics  %>% # take the topics data frame and..
group_by(topic) %>% # treat each topic as a different group
top_n(10, beta) %>% # get the top 10 most informative words
ungroup() %>% # ungroup
arrange(topic, -beta) # arrange words in descending informativeness
top_terms
top_terms %>% # take the top terms
mutate(term = reorder(term, beta)) %>% # sort terms by beta value
ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
geom_col(show.legend = FALSE) + # as a bar plot
facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
labs(x = NULL, y = "Beta") + # no x label, change y label
coord_flip() # turn bars sideways
